{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f111a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import CustomDataset2, confusion_matrix, dice_coeff_batch, DiceLoss\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "from unet import AttU_Net\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "import os, sys\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cda7753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 700.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "#slname = \"\"\n",
    "main_data_dir = '/Users/anuradha.kar/Documents/python_scripts/docker_tests/docker_py1/src/'\n",
    "inf_dir = 'images' ## 132 slide\n",
    "\n",
    "#############\n",
    "pin_memory= True\n",
    "scale_factor = 1.0\n",
    "n_input_channels = 3\n",
    "inf_imgs_fold = []\n",
    "batch_size = 1\n",
    "n_devices = 1\n",
    "n_workers = 0\n",
    "tmp_inf_imgs = natsorted(glob(os.path.join(main_data_dir,inf_dir,'*.png')))\n",
    "\n",
    "               \n",
    "for s in range(len(tmp_inf_imgs)):\n",
    "    inf_imgs_fold.append(tmp_inf_imgs[s])\n",
    "                    \n",
    "inf_dataset = CustomDataset2(inf_imgs_fold, normalize=False,cached_data=pin_memory, n_channels=n_input_channels,scale=scale_factor)\n",
    "inf_loader = DataLoader(inf_dataset, batch_size=batch_size*n_devices, shuffle=False, pin_memory=False, num_workers=n_workers)\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0f0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet.unet_model import AttU_Net\n",
    "\n",
    "n_input_channels = 3\n",
    "\n",
    "    # The number of output classes  (N classes  ->  n_output_channels = N)\n",
    "n_output_channels = 1\n",
    "    #######################################################################\n",
    "    \n",
    "    # defining the U-Net model\n",
    "model = AttU_Net(img_ch=n_input_channels, output_ch=n_output_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87ea0e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chose the GPU CUDA devices to make the training go much faster vs CPU use\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# For faster convolutions, but more memory\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "#model = Unet(inchannels=3, outchannels=1, net_depth=4)\n",
    "# Putting the model inside the device\n",
    "model.to(device=device, dtype=torch.float32)\n",
    "\n",
    "MODEL_PATH = '/Users/anuradha.kar/Documents/python_scripts/unet-pytorch/test_set/best_model_bg10.pth'\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25f0879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:16<00:00,  3.05it/s]\n"
     ]
    }
   ],
   "source": [
    "kernel = np.ones((3, 3), np.uint8)\n",
    "ct= 0\n",
    "ps = 128 \n",
    "all_poly= []\n",
    "for batch in tqdm(inf_loader):\n",
    "    image = batch['image']\n",
    "    fname = tmp_inf_imgs[ct]\n",
    "    fname = os.path.basename(os.path.normpath(fname))\n",
    "    \n",
    "    image = image.to(device=device, dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_mask = model(image)\n",
    "        pred = torch.sigmoid(pred_mask)\n",
    "        pred = (pred > 0.95).float()\n",
    "        pred = pred.cpu()\n",
    "   \n",
    "    pred = Image.fromarray(np.uint8(np.squeeze(pred)*255))\n",
    "    pred= np.asarray(pred)\n",
    "    pred= cv2.dilate(pred, kernel, iterations=1)\n",
    "    \n",
    "    ###### create polygons here\n",
    "    fname= fname.replace('.png','')\n",
    "    x1, y1, x2, y2= float(fname.split('_')[1]), float(fname.split('_')[2]),float(fname.split('_')[3]),float(fname.split('_')[4])\n",
    "    a= (x2-x1)/2\n",
    "    b= (y2-y1)/2\n",
    "    x3= x1 - 64+a  ##bottom left corner of patch\n",
    "    y3= y1- 64+b\n",
    "    x4= x3  ## top left corner which is the origin (0,0) for the patch coordinates\n",
    "    y4= y3-ps\n",
    "    \n",
    "    ret, thresh = cv2.threshold(pred, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    polygons = []\n",
    "    \n",
    "    #############################################\n",
    "    \n",
    "    for object in contours:\n",
    "        coordsx = []\n",
    "        coordsy = []\n",
    "        for point in object:\n",
    "            xval= float(point[0][0])+ float(x4)\n",
    "            yval=float(point[0][1])+ float(y4)+128 ##128 added for shift\n",
    "            coordsx.append(xval) ## add x directly here\n",
    "            coordsy.append(yval) ## add y directly here\n",
    "        coords= list(zip(coordsx, coordsy))\n",
    "        poly=np.array(coords)\n",
    "        if len(poly)> 30:   ##if polygon has more than 10 points``\n",
    "            polygons.append(np.array(coords))\n",
    "    all_poly.append(polygons) \n",
    "      \n",
    "    #cv2.imwrite('/Users/anuradha.kar/Documents/python_scripts/docker_tests/docker_py1/src/preds/'+ fname, pred)\n",
    "    ct+=1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "769c56e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"6656.02 12207.52 39.95999999999913 37.95999999999913\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,24453.0)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"0.7991999999999826\" opacity=\"0.6\" d=\"M 6683.5,12209.0 L 6683.5,12211.0 L 6682.5,12212.0 L 6681.5,12212.0 L 6680.5,12213.0 L 6679.5,12212.0 L 6679.5,12211.0 L 6676.5,12211.0 L 6675.5,12210.0 L 6673.5,12210.0 L 6671.5,12212.0 L 6667.5,12212.0 L 6665.5,12214.0 L 6664.5,12214.0 L 6664.5,12215.0 L 6663.5,12216.0 L 6661.5,12216.0 L 6660.5,12217.0 L 6660.5,12221.0 L 6658.5,12223.0 L 6658.5,12224.0 L 6657.5,12225.0 L 6657.5,12229.0 L 6658.5,12229.0 L 6660.5,12231.0 L 6660.5,12232.0 L 6661.5,12233.0 L 6661.5,12234.0 L 6662.5,12235.0 L 6662.5,12237.0 L 6663.5,12237.0 L 6664.5,12238.0 L 6664.5,12240.0 L 6665.5,12240.0 L 6667.5,12242.0 L 6669.5,12242.0 L 6670.5,12243.0 L 6674.5,12243.0 L 6675.5,12244.0 L 6678.5,12244.0 L 6678.5,12242.0 L 6679.5,12241.0 L 6680.5,12241.0 L 6680.5,12238.0 L 6681.5,12237.0 L 6682.5,12238.0 L 6684.5,12238.0 L 6684.5,12236.0 L 6682.5,12236.0 L 6681.5,12235.0 L 6682.5,12234.0 L 6684.5,12234.0 L 6685.5,12233.0 L 6687.5,12233.0 L 6687.5,12231.0 L 6686.5,12231.0 L 6685.5,12230.0 L 6685.5,12229.0 L 6686.5,12228.0 L 6688.5,12228.0 L 6689.5,12229.0 L 6692.5,12229.0 L 6693.5,12228.0 L 6694.5,12228.0 L 6694.5,12224.0 L 6691.5,12224.0 L 6690.5,12223.0 L 6691.5,12222.0 L 6694.5,12222.0 L 6694.5,12219.0 L 6693.5,12219.0 L 6692.5,12218.0 L 6692.5,12216.0 L 6691.5,12216.0 L 6690.5,12215.0 L 6688.5,12215.0 L 6687.5,12216.0 L 6686.5,12215.0 L 6686.5,12213.0 L 6685.5,12212.0 L 6685.5,12209.0 L 6683.5,12209.0 z\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.polygon.Polygon at 0x7fd26a9446d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from shapely.geometry import Polygon\n",
    "polygon_patch1= Polygon(all_poly[20][0]) ## 1st id- id image, 2nd- id annotation\n",
    "polygon_patch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64d0853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## connect to cytomine\n",
    "## print stuff from cytomine\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "from cytomine import Cytomine\n",
    "#from cytomine.models import *\n",
    "import numpy as np\n",
    "#from skimage import io\n",
    "import string\n",
    "from operator import sub\n",
    "import os\n",
    "\n",
    "from cytomine.models import ProjectCollection\n",
    "#from cytomine.models import AnnotationCollection, ImageInstanceCollection\n",
    "from cytomine.models import StorageCollection\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import os\n",
    "\n",
    "from shapely import wkt\n",
    "from shapely.affinity import affine_transform\n",
    "#from cytomine.models import AnnotationCollection, ImageInstanceCollection\n",
    "from cytomine.models import Property, Project, Annotation, ImageInstance\n",
    "from cytomine.models import Property, Annotation, AnnotationTerm, AnnotationCollection\n",
    "from shapely.geometry import Polygon, LineString\n",
    "from cytomine.models import AnnotationCollection, ImageInstanceCollection\n",
    "from cytomine.models.ontology import Ontology, Term, RelationTerm, TermCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2986b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
