{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuradha.kar/opt/anaconda3/envs/att_unet/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import CustomDataset, confusion_matrix, dice_coeff_batch, DiceLoss\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "from unet import AttU_Net\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os, sys\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 14/14 [00:00<00:00, 300.68it/s]\n"
     ]
    }
   ],
   "source": [
    "inf_imgs_fold = []\n",
    "inf_masks_fold = []\n",
    "\n",
    "pin_memory= True\n",
    "scale_factor = 0.5\n",
    "n_input_channels = 3\n",
    "\n",
    "batch_size = 1\n",
    "n_devices = 1\n",
    "n_workers = 4\n",
    "\n",
    "main_data_dir = '/Users/anuradha.kar/Documents/python_scripts/unet-pytorch/test_set'\n",
    "inf_dir = 'patches'\n",
    "inf_dir_mask = 'masks'\n",
    "\n",
    "ref_image_path = '/Users/anuradha.kar/Documents/python_scripts/unet-pytorch/test_set/patch_2.png'\n",
    "normalize_switch = False\n",
    "\n",
    "tmp_inf_imgs = natsorted(glob(os.path.join(main_data_dir,inf_dir,'*.png')))\n",
    "tmp_inf_masks = natsorted(glob(os.path.join(main_data_dir,inf_dir_mask,'*.png')))\n",
    "\n",
    "for s in range(len(tmp_inf_imgs)):\n",
    "    inf_imgs_fold.append(tmp_inf_imgs[s])\n",
    "    inf_masks_fold.append(tmp_inf_masks[s])\n",
    "\n",
    "inf_dataset = CustomDataset(inf_imgs_fold, inf_masks_fold, ref_image_path, normalize=normalize_switch,cached_data=pin_memory, n_channels=n_input_channels,scale=scale_factor)\n",
    "inf_loader = DataLoader(inf_dataset, batch_size=batch_size*n_devices, shuffle=normalize_switch, pin_memory=pin_memory, num_workers=n_workers)\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the training dataloader\n",
    "\n",
    "main_data_dir = '/Users/anuradha.kar/Documents/python_scripts/unet-pytorch/test_set'\n",
    "inf_dir = 'patches'\n",
    "pin_memory= True\n",
    "scale_factor = 0.5\n",
    "n_input_channels = 3\n",
    "\n",
    "batch_size = 1\n",
    "n_devices = 1\n",
    "n_workers = 4\n",
    "tmp_inf_imgs = natsorted(glob(os.path.join(main_data_dir,inf_dir,'*.png')))\n",
    "\n",
    "               \n",
    "for s in range(len(tmp_inf_imgs)):\n",
    "    inf_imgs_fold.append(tmp_inf_imgs[s])\n",
    "                    \n",
    "inf_dataset = CustomDataset2(inf_imgs_fold, normalize=False,cached_data=pin_memory, n_channels=n_input_channels,scale=scale_factor)\n",
    "inf_loader = DataLoader(inf_dataset, batch_size=batch_size*n_devices, shuffle=False, pin_memory=False, num_workers=n_workers)\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patch_1_C4.png', 'patch_1_C1.png', 'patch_0.png', 'patch_1_C3.png', 'patch_1_C2.png', 'patch_1.png', 'patch_0_C4.png', 'patch_2_C1.png', 'patch_2_C2.png', 'patch_2_C3.png', 'patch_0_C2.png', 'patch_0_C3.png', 'patch_0_C1.png', 'patch_2_C4.png']\n",
      "(64, 64)\n",
      "(64, 64, 3)\n",
      "(64, 64)\n",
      "(64, 64, 3)\n",
      "(64, 64)\n",
      "(64, 64, 3)\n",
      "(64, 64)\n",
      "(64, 64, 3)\n",
      "(64, 64)\n",
      "(64, 64, 3)\n",
      "(64, 64)\n",
      "(64, 64, 3)\n",
      "(64, 64)\n",
      "(64, 64, 3)\n",
      "(64, 64)\n",
      "(64, 64, 3)\n",
      "(64, 64)\n",
      "(64, 64, 3)\n",
      "(64, 64)\n",
      "(64, 64, 3)\n",
      "(64, 64)\n",
      "(64, 64, 3)\n",
      "(64, 64)\n",
      "(64, 64, 3)\n",
      "(64, 64)\n",
      "(64, 64, 3)\n",
      "(64, 64)\n",
      "(64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "path_pred = '/Users/anuradha.kar/Documents/python_scripts/unet-pytorch/test_set/masks/'\n",
    "files = os.listdir(path_pred)\n",
    "#os.remove(path_mask+ '.DS_Store')\n",
    "\n",
    "print(files)\n",
    "for image in files:\n",
    "    img = cv2.imread(os.path.join(path_pred,image), 0)\n",
    "    print(img.shape)\n",
    "    rgb = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
    "    filename = path_pred+image\n",
    "    print(rgb.shape)\n",
    "    cv2.imwrite(filename,rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet.unet_model import AttU_Net\n",
    "\n",
    "n_input_channels = 3\n",
    "\n",
    "    # The number of output classes  (N classes  ->  n_output_channels = N)\n",
    "n_output_channels = 1\n",
    "    #######################################################################\n",
    "    \n",
    "    # defining the U-Net model\n",
    "model = AttU_Net(img_ch=n_input_channels, output_ch=n_output_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chose the GPU CUDA devices to make the training go much faster vs CPU use\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# For faster convolutions, but more memory\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "#model = Unet(inchannels=3, outchannels=1, net_depth=4)\n",
    "# Putting the model inside the device\n",
    "model.to(device=device, dtype=torch.float32)\n",
    "\n",
    "MODEL_PATH = '/Users/anuradha.kar/Documents/python_scripts/unet-pytorch/test_set/best_model.pth'\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████▎                                     | 2/14 [00:04<00:24,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_0.png\n",
      "patch_0_C1.png\n",
      "patch_0_C2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████████▋                            | 5/14 [00:05<00:05,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_0_C3.png\n",
      "patch_0_C4.png\n",
      "patch_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████████▏                  | 8/14 [00:05<00:01,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_1_C1.png\n",
      "patch_1_C2.png\n",
      "patch_1_C3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████▊         | 11/14 [00:05<00:00,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_1_C4.png\n",
      "patch_2_C1.png\n",
      "patch_2_C2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 14/14 [00:06<00:00,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_2_C3.png\n",
      "patch_2_C4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 14/14 [00:26<00:00,  1.87s/it]\n"
     ]
    }
   ],
   "source": [
    "ct= 0\n",
    "for batch in tqdm(inf_loader):\n",
    "    image = batch['image']\n",
    "    fname = tmp_inf_imgs[ct]\n",
    "    fname = os.path.basename(os.path.normpath(fname))\n",
    "    \n",
    "    image = image.to(device=device, dtype=torch.float32)\n",
    "    #img=Image.open(filename).convert('RGB')\n",
    "    #img = np.array(img)\n",
    "    #img = img.transpose((2, 0, 1))\n",
    "    #img = np.expand_dims(img,3)\n",
    "    #img = img.transpose((3, 0, 1, 2))\n",
    "    #img = torch.from_numpy(img)\n",
    "    #img = img.to(device, dtype=torch.float)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_mask = model(image)\n",
    "        pred = torch.sigmoid(pred_mask)\n",
    "        pred = (pred > 0.5).float()\n",
    "        pred = pred.cpu()\n",
    "        #pred= np.array(np.squeeze(pred))\n",
    "        # print(pred.shape)\n",
    "    #pred = np.uint8(np.squeeze(pred))\n",
    "    pred = Image.fromarray(np.uint8(np.squeeze(pred)*255))\n",
    "    \n",
    "    #np.save('/Users/anuradha.kar/Documents/python_scripts/'+fname+ '_newlabels.npy', new_labels)\n",
    "    pred.save('/Users/anuradha.kar/Documents/python_scripts/unet-pytorch/test_set/pred/'+ fname)\n",
    "    #plt.figure()\n",
    "    #plt.imshow(pred)\n",
    "    #plt.show()\n",
    "    print(fname)\n",
    "    ct+=1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m pil_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/anuradha.kar/Documents/python_scripts/unet-pytorch/test_set/pred/patch_0_C1.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m pil_img\u001b[38;5;241m.\u001b[39mputalpha(\u001b[43mL\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#r, g, b, a= pil_img.split()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(pil_img)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'L' is not defined"
     ]
    }
   ],
   "source": [
    "pil_img = Image.open('/Users/anuradha.kar/Documents/python_scripts/unet-pytorch/test_set/pred/patch_0_C1.png')\n",
    "pil_img.putalpha(L)\n",
    "\n",
    "#r, g, b, a= pil_img.split()\n",
    "plt.imshow(pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     predmask_fold\u001b[38;5;241m.\u001b[39mappend(tmp_pred_masks[s])\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#comparison dataset and loader -- image = true mask, mask = pred mask\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m comp_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruemask_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpredmask_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_switch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcached_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_input_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m comp_loader \u001b[38;5;241m=\u001b[39m DataLoader(comp_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size\u001b[38;5;241m*\u001b[39mn_devices, shuffle\u001b[38;5;241m=\u001b[39mnormalize_switch, pin_memory\u001b[38;5;241m=\u001b[39mpin_memory, num_workers\u001b[38;5;241m=\u001b[39mn_workers)\n",
      "File \u001b[0;32m~/Documents/python_scripts/unet-pytorch/attnetion_unet/utils/help_functions.py:276\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[0;34m(self, imgs_dirs, masks_dirs, ref_image_path, normalize, cached_data, n_channels, scale)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mlen\u001b[39m(imgs_dirs)):\n\u001b[1;32m    275\u001b[0m     pil_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs_dirs[i])\n\u001b[0;32m--> 276\u001b[0m     np_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpil_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_imgs\u001b[38;5;241m.\u001b[39mappend(np_img)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks_dirs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs_dirs):\n",
      "File \u001b[0;32m~/Documents/python_scripts/unet-pytorch/attnetion_unet/utils/help_functions.py:301\u001b[0m, in \u001b[0;36mCustomDataset.preprocess\u001b[0;34m(self, pil_img, ref_image, n_channels, scale, normalize, mask)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m(mask):\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(pil_img)\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m n_channels:\n\u001b[0;32m--> 301\u001b[0m         r, g, b, a \u001b[38;5;241m=\u001b[39m pil_img\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m    302\u001b[0m         pil_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mmerge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m, (r,g,b))\n\u001b[1;32m    304\u001b[0m      \u001b[38;5;66;03m# This part is concerns the normalization \u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "#create data loader for pred and true mask\n",
    "\n",
    "truemask_fold = []\n",
    "predmask_fold = []\n",
    "\n",
    "pin_memory= True\n",
    "scale_factor = 0.5\n",
    "n_input_channels = 3\n",
    "\n",
    "batch_size = 1\n",
    "n_devices = 1\n",
    "n_workers = 4\n",
    "\n",
    "main_data_dir = '/Users/anuradha.kar/Documents/python_scripts/unet-pytorch/test_set'\n",
    "pred_dir = 'pred'\n",
    "true_mask = 'masks'\n",
    "\n",
    "ref_image_path = '/Users/anuradha.kar/Documents/python_scripts/unet-pytorch/test_set/patch_2.png'\n",
    "normalize_switch = False\n",
    "\n",
    "tmp_true_masks = natsorted(glob(os.path.join(main_data_dir,true_mask,'*.png')))\n",
    "tmp_pred_masks = natsorted(glob(os.path.join(main_data_dir,pred_dir,'*.png')))\n",
    "\n",
    "for s in range(len(tmp_true_masks)):\n",
    "    truemask_fold.append(tmp_true_masks[s])\n",
    "    predmask_fold.append(tmp_pred_masks[s])\n",
    "\n",
    "    #comparison dataset and loader -- image = true mask, mask = pred mask\n",
    "comp_dataset = CustomDataset(truemask_fold,predmask_fold, ref_image_path, normalize=normalize_switch,cached_data=pin_memory, n_channels=n_input_channels,scale=scale_factor)\n",
    "comp_loader = DataLoader(comp_dataset, batch_size=batch_size*n_devices, shuffle=normalize_switch, pin_memory=pin_memory, num_workers=n_workers)\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im= Image.fromarray(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = im.size\n",
    "print(width,height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
